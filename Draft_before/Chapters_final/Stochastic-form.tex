% !TEX root = ./sum1.tex

\section{Seat Planning with Stochastic Requests}\label{sec_seat_planning}
We now investigate the problem of seat planning with stochastic requests. Specifically, we need to determine a seating plan that accommodates uncertain requests. To model this problem, we develop a scenario-based stochastic programming (SBSP) and follow the framework of Benders decomposition to solve it. %it efficiently. In some cases, solving the integer programming with Benders decomposition remains still computationally prohibitive. Thus, we can consider the LP relaxation first, then construct a seat plan with full or largest patterns to fully utilize all seats.


\subsection{Scenario-Based Stochastic Programming Formulation}
Assume the requests can be defined as a set of scenarios, denoted by  $\omega \in \Omega$. Each scenario $\omega$ is associated with a specific realization of group requests, represented as $\mathbf{d}_\omega = (d_{1\omega},d_{2\omega},\ldots,d_{M,\omega})^{\intercal}$, and  $p_{\omega}$, the realization probability of the scenario $\omega$. To maximize the expected number of individuals accommodated across all scenarios, we propose a scenario-based stochastic programming approach to determine a seat plan.

Recall that $x_{ij}$ given in \eqref{e0} represents the number of groups of type $i$ planned in row $j$. To account for the variability across different scenarios, it is essential to model potential excess or shortage of supply. To capture this, we introduce a scenario-dependent decision variable, denoted as $\mathbf{y}$, which consists of two vectors: $\mathbf{y}^{+} \in \mathbb{N}^{M \times |\Omega|}$ and $\mathbf{y}^{-} \in \mathbb{N}^{M \times |\Omega|}$. Here, each component of $\mathbf{y}^{+}$, denoted as $y_{i\omega}^{+}$, represents the excess of supply for group type $i$ under scenario $\omega$, while $y_{i\omega}^{-}$ represents the shortage of supply for group type $i$ under scenario $\omega$.

To address the possibility of larger group types being unable to fully occupy their designated seats due to insufficient supply, we assume that surplus seats for group type $i$ can be 
allocated to smaller group types $j<i$ in descending order of group size. This implies that if there is excess supply after assigning groups of type $i$ to rows, the remaining seats can be hierarchically allocated to groups of type $j<i$ based on their sizes. Recall that the supply for group type $i$ is denoted as $\sum_{j=1}^N x_{ij}$. Thus, for any scenario $\omega$, the excess and shortage of supply can be recursively defined as follows:

\begin{equation}\label{y_recursively}
\begin{aligned}
  & y_{i \omega}^{+}= (\sum_{j=1}^N x_{ij}- d_{i \omega} + y_{i+1, \omega}^{+})^{+}, i =1,\ldots, M-1 \\
  & y_{i \omega}^{-}= (d_{i \omega}- \sum_{j=1}^N x_{ij} - y_{i+1, \omega}^{+})^{+}, i =1,\ldots, M-1 \\
  & y_{M \omega}^{+} = (\sum_{j=1}^N x_{Mj} - d_{M \omega})^{+} \\ 
  & y_{M \omega}^{-} = (d_{M \omega}- \sum_{j=1}^N x_{Mj})^{+},
\end{aligned}
\end{equation}
where $(\cdot)^{+}$ denotes the non-negative part of the expression. 

Based on the considerations outlined above, the total supply of group type $i$ under scenario $\omega$ can be expressed as: $\sum_{j= 1}^{N} x_{ij} + y_{i+1,\omega}^{+} - y_{i \omega}^{+}, i = 1, \ldots, M-1$. For the special case of group type $M$, the total supply under scenario $\omega$ is $\sum_{j= 1}^{N} x_{Mj} - y_{M \omega}^{+}$.
Then we have the following formulation:
  \begin{align}
  (\text{SBSP}) \quad \max \quad & E_{\omega}\left[(n_{M}-\delta) (\sum_{j= 1}^{N} x_{Mj} - y_{M \omega}^{+}) + \sum_{i=1}^{M-1} (n_i-\delta) (\sum_{j= 1}^{N} x_{ij} + y_{i+1,\omega}^{+} - y_{i \omega}^{+})\right] \label{obj_sto}\\
  \text {s.t.} \quad & \sum_{j= 1}^{N} x_{ij}-y_{i \omega}^{+}+
  y_{i+1, \omega}^{+} + y_{i \omega}^{-}=d_{i \omega}, \quad i = 1,\ldots, M-1, \omega \in \Omega \label{DEF_constr1} \\
  & \sum_{j= 1}^{N} x_{ij} -y_{i \omega}^{+}+y_{i \omega}^{-}=d_{i \omega}, \quad i = M, \omega \in \Omega \label{DEF_constr2}\\
  & \sum_{i=1}^{M} n_{i} x_{ij} \leq L_j, j \in \mathcal{N}  \label{DEF_constr3} \\
  & y_{i \omega}^{+}, y_{i \omega}^{-} \in \mathbb{N}, \quad i \in \mathcal{M}, \omega \in \Omega \notag \\
  & x_{ij} \in \mathbb{N}, \quad i \in \mathcal{M}, j \in \mathcal{N} \notag.
  \end{align}


The objective function consists of two parts. The first part represents the number of individuals in
group type $M$ that can be accommodated, given by $(n_{M}-\delta) (\sum_{j=1}^{N} x_{Mj} - y_{M\omega}^{+})$. The second part represents the number of individuals in group type $i$, excluding $M$, that can be accommodated, given by $(n_i-\delta) (\sum_{j=1}^{N} x_{ij} + y_{i+1,\omega}^{+} - y_{i\omega}^{+}), i = 1, \ldots, M-1$. The overall objective function is subject to an expectation operator denoted by $E_{\omega}$, which represents the expectation with respect to the scenario set. This implies that the objective function is evaluated by considering the average values of the decision variables and constraints over the different scenarios. The objective function can be then reformulated as follows.
\begin{align*}
  & E_{\omega}\left[\sum_{i=1}^{M-1} (n_i-\delta) (\sum_{j= 1}^{N} x_{ij} + y_{i+1,\omega}^{+} - y_{i \omega}^{+}) + (n_M-\delta) (\sum_{j= 1}^{N} x_{Mj} - y_{M \omega}^{+})\right] \\
  = & \sum_{j =1}^{N} \sum_{i=1}^M (n_i- \delta) x_{ij} - \sum_{\omega \in \Omega} p_{\omega} \left(\sum_{i=1}^{M}(n_i- \delta)y_{i \omega}^{+} - \sum_{i=1}^{M-1}(n_i-\delta)y_{i+1, \omega}^{+}\right) \\
  = & \sum_{j =1}^{N} \sum_{i=1}^M i \cdot x_{ij} - \sum_{\omega \in \Omega} p_{\omega} \sum_{i = 1}^{M} y_{i \omega}^{+}
\end{align*}

Here, $\sum_{j =1}^{N} \sum_{i=1}^M i \cdot x_{ij}$ indicates the maximum number of individuals that can be accommodated in the seat plan $\{x_{ij}\}$. The second part, $\sum_{\omega \in \Omega} p_{\omega} \sum_{i = 1}^{M} y_{i \omega}^{+}$ indicates the expected excess of supply for group type $i$ over scenarios.

In the optimal solution, at most one of $y_{i \omega}^{+}$ and $y_{i \omega}^{-}$ can be positive for any $i, \omega$. Suppose there exist $i_0$ and $\omega_0$ such that $y_{i_0, \omega_0}^{+}$ and $y_{i_0, \omega_0}^{-}$ are positive. Substracting $\min\{y_{i_0, \omega_0}^{+}, y_{i_0, \omega_0}^{-}\}$ from these two values will still satisfy constraints \eqref{DEF_constr1} and \eqref{DEF_constr2} but increase the objective value when $p_{\omega_0}$ is positive. Thus, in the optimal solution, at most one of $y_{i \omega}^{+}$ and $y_{i \omega}^{-}$ can be positive.

% Considering the analysis provided earlier, we find it advantageous to obtain a seat plan that only consists of full or largest patterns. However, the seat plan associated with the optimal solution obtained by solver to SSP may not consist of the largest or full patterns. We can convert the optimal solution to another optimal solution which is composed of the largest or full patterns.

\begin{prop}\label{prop_solution}
There exists an optimal solution to \text{SBSP} given in \eqref{obj_sto} such that the patterns associated with this optimal solution are composed of the full or largest patterns under any given scenarios.
\end{prop}

When there is only one scenario, SBSP reduces to the deterministic model. This aligns with Section \ref{seat_planning_full_largest}, which outlines the generation of seat plan consisting of full or largest patterns.

Solving SBSP directly is computationally prohibitive when there are numerous scenarios, instead, we apply Benders decomposition to simplify the solving process in Section \ref{solve_by_benders}, then obtain the seat plan composed of full or largest patterns, as stated in Section \ref{seat_assignment}.

\subsection{Solving SBSP via Benders Decomposition}\label{solve_by_benders}
We use Benders decomposition approach \citep{bnnobrs1962partitioning} to solve SBSP. The core idea is to decompose the SBSP problem into a master problem and a set of subproblems. By iteratively solving the master problem and subproblems, gradually approaching the optimal solution of the original problem. We utilize the closed-form solution to the subproblems to expedite the solving process.

\subsubsection{Master Problem Formulation}
We express the SBSP formulation in matrix form to enable efficient Benders decomposition. 
Let $\mathbf{n} = (n_1, \ldots, n_M)^{\intercal}$ denote group sizes and $\mathbf{L} = (L_1, \ldots, L_N)^{\intercal}$ represent row sizes. The capacity constraint \eqref{DEF_constr3} becomes: $\mathbf{x}^{\intercal} \mathbf{n} \leq \mathbf{L}$. We can use the product $\mathbf{x} \mathbf{1}$ to indicate the supply of group types, where $\mathbf{1}$ is a column vector of size $N$ with all elements equal to 1. Constraints \eqref{DEF_constr1} and \eqref{DEF_constr2} can be expressed in the matrix form as:
\[\mathbf{x} \mathbf{1} + \mathbf{V} \mathbf{y}_\omega = \mathbf{d}_\omega, \omega\in \Omega,\]
with the block matrix $\mathbf{V} = [\mathbf{W}, ~\mathbf{I}]$, where $\mathbf{W}$ is an $M \times M$ lower-bidiagonal matrix:

$$
\mathbf{W}=\left[\begin{array}{cccccc}
-1 & 1 & 0 & \ldots & \ldots & 0 \\
0 & -1 & 1 &    0   & \ldots & 0 \\
\vdots & \ddots & \ddots & \ddots & \ddots & \vdots \\
0  & \ldots   &  0  & -1 & 1 & 0 \\
0  & \ldots   &  \ldots  &  0 &  -1 & 1 \\
0 & \ldots & \ldots & \ldots & 0 & -1
\end{array}\right]
$$
and $\mathbf{I}$ is the M-dimensional identity matrix. The scenario-dependent variables are structured as:

$$
\mathbf{y}_{\omega}=\left[\begin{array}{l}
\mathbf{y}_{\omega}^{+} \\
\mathbf{y}_{\omega}^{-}
\end{array}\right], \mathbf{y}_{\omega}^{+}=\left[\begin{array}{lllll}y_{1 \omega}^{+} & y_{2 \omega}^{+} & \cdots & y_{M \omega}^{+}\end{array}\right]^{\intercal}, \mathbf{y}_{\omega}^{-}=\left[\begin{array}{llll}y_{1 \omega}^{-} & y_{2 \omega}^{-} & \cdots & y_{M \omega}^{-}\end{array}\right]^{\intercal}.
$$

The objective components of the SBSP formulation are: $$\mathbf{c}^{\intercal}\mathbf{x} = \sum_{j =1}^{N} \sum_{i=1}^M i \cdot x_{ij}, \quad \mathbf{f}^{\intercal}\mathbf{y}_{\omega} = -\sum_{i=1}^{M} y_{i \omega}^{+}.$$ 

The master problem formulation is:

\begin{equation}\label{BD_master1}
  \begin{aligned}
    \max \quad & \mathbf{c}^{\intercal} \mathbf{x} + \sum_{\omega \in \Omega} p_{\omega} z_{\omega} \\
    \text {s.t.} \quad & \mathbf{x}^{\intercal} \mathbf{n}  \leq \mathbf{L} \\
    & \{\text{Cuts to be added by the subproblem} \} \\
    & \mathbf{x} \in \mathbb{N}^{M \times N}, z_{\omega} \in \mathbb{R}
  \end{aligned}
\end{equation}

The constraints to be incorporated from the subproblem \eqref{BD_sub} remain to be added through Benders decomposition. Here, $z_{\omega}$ serves as a lower bound approximation for the optimal value 
$z_{\omega}(\mathbf{x})$ of the subproblem. The introduction of $z_{\omega}$ enables decomposition of the original problem into: a master problem involving the $\mathbf{x}$ variables and a set of subproblems (one per scenario $\omega$) involving the $\mathbf{y}_{\omega}$ variables.

% This decomposition scheme allows us to generate Benders cuts to iteratively improve the approximation of $z_{\omega}(\mathbf{x})$. The lower bound property $z_{\omega} \leq z_{\omega}(\mathbf{x})$ ensures valid cuts are added to progressively tighten the master problem's feasible region.

\subsubsection{Subproblem Analysis and Benders Cuts}\label{second_stage}
For each scenario $\omega$, we have the subproblem:

\begin{equation}\label{BD_sub}
  \begin{aligned}
    z_{\omega}(\mathbf{x}) = \max \quad & \mathbf{f}^{\intercal} \mathbf{y}_{\omega} \\
    \text {s.t.} \quad & \mathbf{V} \mathbf{y}_{\omega} = \mathbf{d}_{\omega} - \mathbf{x} \mathbf{1} \\
     & \mathbf{y}_{\omega} \geq 0.
  \end{aligned}
\end{equation}


Let $\bm{\alpha}_{\omega} = (\alpha_{1\omega},\alpha_{2\omega}, \ldots, \alpha_{M,\omega})^{\intercal}$ denote the vector of dual variables. For each $\omega$, the dual of the subproblem can be expressed as:

\begin{equation}\label{BD_sub_dual}
  \begin{aligned}
    \min \quad & \bm{\alpha}_{\omega}^{\intercal} (\mathbf{d}_{\omega}- \mathbf{x} \mathbf{1}) \\
    \text {s.t.} \quad & \bm{\alpha}_{\omega}^{\intercal} \mathbf{V} \geq \mathbf{f}^{\intercal}
  \end{aligned}
\end{equation}

\begin{lem}\label{feasible_region}
 The feasible region of problem \eqref{BD_sub_dual} is nonempty and bounded. Furthermore, all the extreme points of the feasible region are integral.
\end{lem}


Given $\mathbf{x}$, $\mathbf{y}_{\omega}$ can be obtained from equation \eqref{y_recursively}. Let $\alpha_{0, \omega} = 0$ for each $\omega$, then we have Proposition \ref{optimal_sol_sub_dual}.

\begin{prop}\label{optimal_sol_sub_dual}
  The optimal solutions to problem \eqref{BD_sub_dual} are given by 
\begin{equation}\label{BD_sub_simplified}
  \begin{aligned}
    \alpha_{i \omega} = 0 \quad & \text{if}~  y_{i \omega}^{-} > 0,  i =1,\ldots, M~\text{or}~ y_{i \omega}^{-} = y_{i \omega}^{+} = 0, y_{i+1, \omega}^{+}> 0, i = 1,\ldots, M-1 \\
    \alpha_{i \omega} = \alpha_{i-1, \omega}+1 \quad & \text{if}~ y_{i \omega}^{+} > 0, i =1,\ldots, M \\
    0 \leq \alpha_{i \omega} \leq \alpha_{i-1, \omega}+1 \quad & \text{if}~ y_{i \omega}^{-} = y_{i \omega}^{+} = 0, i = M~\text{or}~ y_{i \omega}^{-} = y_{i \omega}^{+} = 0, y_{i+1, \omega}^{+}= 0, i = 1,\ldots, M-1
  \end{aligned}
\end{equation}
\end{prop}


We can obtain $\alpha_{i \omega}$ through a forward calculation, iterating from $\alpha_{1 \omega}$ to $\alpha_{M\omega}$. In practice, we choose $\alpha_{i \omega} = \alpha_{i-1, \omega}+1$ when $y_{i \omega}^{-}, y_{i \omega}^{+}$ satisfy the third condition in Proposition \ref{optimal_sol_sub_dual}.


The Benders decomposition algorithm iteratively adds two types of cutting planes to the master problem.
The \textit{optimality cuts} provide the approximation of the subproblem's optimal solution $z_{\omega}(\mathbf{x})$, constructed using dual solutions from feasible subproblem instances. They progressively improve the lower bound approximation of $z_{\omega}(\mathbf{x})$. The \textit{feasibility cuts} enforce primal feasibility conditions for the subproblem, eliminating master problem solutions that would lead to infeasible subproblem instances \citep{rahmaniani2017benders}.

In our problem, Lemma \ref{feasible_region} establishes that the optimal value $z_{\omega}(\mathbf{x})$ of subproblem \eqref{BD_sub} is always finite, implying that only optimality cuts are necessary. Let $\mathbb{P}$ denote the feasible region of problem $\eqref{BD_sub_dual}$ and $\mathcal{O}$ represent the set of extreme points of $\mathbb{P}$. 

The subproblem's optimal value can then be characterized as: $z_{\omega}(\mathbf{x}) = \min_{\bm{\alpha}_{\omega} \in \mathcal{O}} \bm{\alpha}_{\omega}^{\intercal}(\mathbf{d}_{\omega}- \mathbf{x} \mathbf{1})$. This leads to the optimality cuts for the master problem: $\bm{\alpha}_{\omega}^{\intercal}(\mathbf{d}_{\omega}- \mathbf{x} \mathbf{1}) \geq z_w, \forall \bm{\alpha}_{\omega} \in \mathcal{O}$, where $z_w$ is the variable in master problem \eqref{BD_master1} that approximates $z_{\omega}(\mathbf{x})$.



% Let $z_{\omega}(\mathbf{x})$ be the largest number $z_{\omega}$ such that $\bm{\alpha}_{\omega}^{\intercal}(\mathbf{d}_{\omega}- \mathbf{x} \mathbf{1}) \geq z_w, \forall \bm{\alpha}_{\omega} \in \mathcal{O}$. 

\subsubsection{Iterative Algorithm}
Let $\mathcal{O}^{s}$ denote the subset of $\mathcal{O}$. The master problem can thus be put in the following form:

\begin{equation}\label{BD_master2}
  \begin{aligned}
    \max \quad & \mathbf{c}^{\intercal} \mathbf{x} + \sum_{\omega \in \Omega} p_{\omega} z_{\omega} \\
    \text {s.t.} \quad & \mathbf{x}^{\intercal} \mathbf{n}  \leq \mathbf{L} \\
    & \bm{\alpha}_{\omega}^{\intercal}(\mathbf{d}_{\omega}- \mathbf{x} \mathbf{1}) \geq z_{\omega}, \forall \bm{\alpha}_{\omega} \in \mathcal{O}^{s}, \forall \omega \\
     & \mathbf{x} \in \mathbb{N}^{M \times N}, z_{\omega} \in \mathbb{R}, \forall \omega
  \end{aligned}
\end{equation}


To find the optimal solution to the SBSP problem, we iteratively solve the master problem \eqref{BD_master2} and incrementally adding more constraints until the optimal solution to the original problem is obtained.


An initial $\mathcal{O}^{s}$ can be given with any extreme points of $\mathbb{P}$, in practice, we choose $\bm{\alpha}_{\omega} = \mathbf{0}, \forall \omega$. Given the initial $\mathcal{O}^{t}$, we can obtain the solution $\mathbf{x}^{*}$ and $\mathbf{z}^{*} =(z^{*}_1,\ldots, z^{*}_{|\Omega|})$. Then $c^{\intercal} \mathbf{x}^{*} + \sum_{\omega \in \Omega} p_{\omega} z_{\omega}^{*}$ is an upper bound of the SBSP problem (UB). When $\mathbf{x}^{*}$ is given, the optimal solution, $\bm{\tilde{\alpha}}_{\omega}$, to problem \eqref{BD_sub_dual} can be obtained according to Proposition \ref{optimal_sol_sub_dual}. Let $\tilde{z}_{\omega} = \bm{\tilde{\alpha}}_{\omega}(d_{\omega} - \mathbf{x}^{*} \mathbf{1})$, then $(\mathbf{x}^{*}, \mathbf{\tilde{z}})$ is a feasible solution to the original problem because it satisfies all the constraints. Thus, $\mathbf{c}^{\intercal} \mathbf{x}^{*} + \sum_{\omega \in \Omega} p_{\omega} \tilde{z}_{\omega}$ is a lower bound of the SBSP problem (LB).


If, for every scenario $\omega$, the optimal value of the problem \eqref{BD_sub_dual} is larger than or equal to $z_{\omega}^{*}$, which means that all contraints in SBSP are satisfied, then we have an optimal solution $(\mathbf{x}^{*}, \mathbf{z}^{*})$ to the SBSP problem. However, if there exists at least one scenario $\omega$ for which the optimal value of problem \eqref{BD_sub_dual} is less than $z_{\omega}^{*}$, indicating that the constraints are not fully satisfied, we need to add a new constraint $(\bm{\tilde{\alpha}}_{\omega})^{\intercal}(\mathbf{d}_{\omega} - \mathbf{x} \mathbf{1}) \geq z_{\omega}$ to the master problem \eqref{BD_master2}.

Note that in each iteration, only constraints (Benders cuts) are added to the master problem. Consequently, UB decreases monotonically with each iteration, as each new cut either tightens the feasible region or maintains the current bound.
While the lower bound (LB) is guaranteed to converge to the optimal value, its monotonicity depends on the initial cuts:
if the master problem starts with no cuts (or loose cuts), LB increases monotonically as cuts are added; if a strong priori cuts are used, LB may non-monotonically approach the optimal value. In our implementation, the initial cuts $\bm{\alpha}_{\omega} = \mathbf{0}, \forall \omega$ cannot ensure the monotonic LB growth.

The gap $\textup{UB} - \textup{LB}$ is guaranteed to converge to zero within finite iterations because the dual feasible region $\mathbb{P}$ has a finite number of extreme points and each iteration adds the new optimality cuts corresponding to the previously unexplored extreme points. This convergence property follows from the finite nature of $\mathcal{O}$. The algorithm terminates when $\textup{UB} - \textup{LB} < \epsilon$ for some prescribed tolerance $\epsilon >0$.

\begin{algorithm}[h]
  \caption{Benders Decomposition}\label{cut_algo}
  % \KwIn{Initial problem \eqref{BD_master3} with $\bm{\alpha}_{\omega} = 0, \forall \omega$, $LB = 0$, $UB = \infty$, $\epsilon$.}
  % \KwOut{$\mathbf{x}^{*}$}
  Initialize $\bm{\alpha}_{\omega} = \mathbf{0}, \forall \omega$, $\epsilon >0$, and let $LB \gets 0$, $UB \gets \infty$\;
  \While{$UB - LB > \epsilon$}
    {Solve problem \eqref{BD_master2} and obtain an optimal solution $(\mathbf{x}^{*}, \mathbf{z}^{*})$\;
    $UB \gets c^{\intercal} \mathbf{x}^{*} + \sum_{\omega \in \Omega} p_{\omega} z_{\omega}^{*}$\;
    \For{$\omega= 1, \ldots, |\Omega|$}
    {Obtain $\bm{\tilde{\alpha}}_{\omega}$ according to Proposition \ref{optimal_sol_sub_dual}\; $\tilde{z}_{\omega}= (\bm{\tilde{\alpha}}_{\omega})^{\intercal}(\mathbf{d}_{\omega}- \mathbf{x}^{*} \mathbf{1})$\;
    \If{$\tilde{z}_{\omega} < z_{\omega}^{*}$}
    {Add one new constraint, $(\bm{\tilde{\alpha}}_{\omega})^{\intercal}(\mathbf{d}_{\omega}- \mathbf{x} \mathbf{1}) \geq z_{\omega}$, to problem \eqref{BD_master2}\;}
    }
    {$LB \gets c^{\intercal} \mathbf{x}^{*} + \sum_{\omega \in \Omega} p_{\omega} \tilde{z}_{\omega} $\;}
    }
\end{algorithm}

However, solving problem \eqref{BD_master2} iteratively with integrality constraints can be computationally intensive in certain cases. To address this, we adopt a two-phase approach: first, we solve the LP relaxation of the SBSP problem using Benders decomposition to obtain a fractional solution; from this relaxed solution, we then generate a seat plan consisting of full or largest patterns.

\subsection{Obtaining Seat Plan with Full or Largest Patterns}\label{seat_assignment}
We may obtain a fractional optimal solution when we solve the LP relaxation of the SBSP problem. This solution represents the optimal allocations of groups to seats but may involve fractional values, indicating partial assignments. Based on the fractional solution obtained, we use the deterministic model to generate a feasible seat plan. The objective of this model is to allocate groups to seats in a way that satisfies the supply requirements for each group without exceeding the corresponding supply values obtained from the fractional solution. To accommodate more groups and optimize seat utilization, we aim to construct a seat plan composed of full or largest patterns based on the feasible seat plan obtained in the previous step. 


Let $\mathbf{x}^{*}$ denote the optimal solution to the LP relaxation of the SBSP problem. Aggregate $\mathbf{x}^{*}$ to the number of each group type, defined as $\tilde{X}_{i} =\sum_{j} x^{*}_{ij}, \forall i \in \mathbf{M}$. Next, solve the SPDR problem with $\bm{d} = \bm{\tilde{X}}$ to obtain the optimal solution $\mathbf{\tilde{x}}$, and the corresponding pattern $\tilde{\bm{H}}$. Then generate the seat plan by problem \eqref{improve_seat} with $\bm{H}=  \tilde{\bm{H}}$.


\begin{algorithm}
  \caption{Seat Plan Construction}\label{seat_construction}
  % \KwIn{Scenarios set $\Omega$, Seat layout $\bm{L}$}
  % \KwOut{$\bm{H}^{\prime}$}
    {Solve the LP relaxation of SBSP in \eqref{obj_sto}, and obtain an optimal solution $\mathbf{x}^{*}$\;}
    {Solve the SPDR problem in \eqref{e0} with $d_{i} = \sum_{j} x^{*}_{ij}, i \in \mathbf{M}$, and obtain an optimal solution $\tilde{\mathbf{x}}$ and the corresponding pattern, $\tilde{\bm{H}}$\;}
    {Solve problem \eqref{improve_seat} with $\bm{H} = \tilde{\bm{H}}$, and obtain the seat plan $\bm{H}^{\prime}$\;}
\end{algorithm}

% {\color{red}{do we need algorithm 2? The description above seems clear enough.}}
