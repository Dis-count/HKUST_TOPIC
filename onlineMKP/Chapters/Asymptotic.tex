% !TEX root = sum1.tex
\section{Evaluation on the Asymptotic Loss}
In this section, we examine the notion of loss in revenue management. To establish benchmarks, we define two reference values for the base problem instance. The first is the offline optimal value $V^{\text{OFF}}$, which is defined as the expected value of the following integer program (IP):

\begin{equation}\label{offline_IP}
    \begin{aligned}
        z(\bm{d}) = \quad \max \quad & \sum_{i=1}^{M}  \sum_{j= 1}^{N} r_{i} x_{ij} \\
        \text {s.t.} \quad & \sum_{j= 1}^{N} x_{ij} \leq d_{i}, \quad i \in \mathcal{M}, \\ 
        & \sum_{i=1}^{M} w_{i} x_{ij} \leq c_j, j \in \mathcal{N},  \\
        & x_{ij} \in \mathbb{N}, \quad i \in \mathcal{M}, j \in \mathcal{N}. 
    \end{aligned}
\end{equation}

Here $V^{\text{OFF}} = E(z(\bm{d}))$, representing the maximum expected revenue attainable with perfect knowledge of all future demand realizations.

The second benchmark is the offline relaxed optimal value $V^{\text{HO}}$, which is defined as the optimal value of the same offline problem but with the integrality constraints $x_{ij} \in \mathbb{N}$ relaxed to $x_{ij} \geq 0$. By definition, we have $V^{\text{HO}} \geq V^{\text{OFF}}$. For any policy $\pi$, let $V^{\pi}$ denote the expected revenue collected under $\pi$. The expected loss of a policy $\pi$ is therefore bounded by
$E[\text{loss}^{\pi}] = V^{\text{OFF}} - V^{\pi} \leq V^{\text{HO}} - V^{\pi}$. To obtain a tractable upper bound, we will measure the loss of policy $\pi$ through the difference $V^{\text{HO}} - V^{\pi}$.


Direct analysis of this loss term, however, is often intractable. To evaluate policy performance systematically, we adopt the standard asymptotic scaling framework prevalent in revenue management literature (see \citet{gallego1997multiproduct}). Let $\theta \in \mathbb{N}$ be a scale parameter. For a given base instance, we generate a sequence of scaled problems. In the $\theta$-th problem, the resource capacity vector is scaled to $\bm{C}(\theta) = \theta \bm{C}$, and the selling horizon is extended to $\theta T$ periods. The arrival rates are defined as $\lambda_i^t(\theta) = \lambda_i^{\lceil t/\theta \rceil}$ for $t \in [\theta T]$. 

Let $V_{\theta}^{\pi}$ be the expected revenue of policy $\pi$ in the scaled problem. Within this framework, we will study the asymptotic behavior of the loss $V^{\text{HO}}_{\theta}- V^{\pi}_{\theta}$ as the scale parameter $\theta \to \infty$. This provides the core metric for assessing the asymptotic optimality of policy $\pi$. For the DPP policy, we have Proposition \ref{loss_dpp}.

% HO makes an allocation decision only after all requests are known, and thus achieves the optimal value for the relaxed static model.

\begin{prop}\label{loss_dpp}
When $\theta \to \infty$, we have $V_{\theta}^{\text{HO}} - V_{\theta}^{\text{DPP}} = O(1)$. 
\end{prop}

We now outline the proof of Proposition \ref{loss_dpp}. First, we perform a loss decomposition. Starting from a modified version of \eqref{improve_primal} that imposes lower bounds on $\bm{x}$, we decompose--for each sample path--the total loss between the DPP and the hindsight optimum into $\theta T$ increments. Each increment corresponds to the gap between two objective values of the modified problem under different lower bounds for $\bm{x}$. Second, we establish a uniform bound on the single-period loss. Each increment is shown to be bounded above by a constant that depends only on ${r_i}$. The third step is we bound the probability of loss. Finally, we sum the increments over the horizon $T$. This sum converges to a constant, completing the proof.

% Then we upper bound each increment. We show that

% Step 1: Loss Decomposition
% Step 2: Bounding the Single-Period Loss
% Step 3: Bounding the Probability of Loss
% Step 4: Summing Over Time

In the first step, let $\gamma_{ij}$, $\gamma_{i0}$ denote the number of type $i$ accepted in capacity $j$ and rejected by DPP, respectively. We consider the following variant of \eqref{improve_primal}.

\begin{equation}\label{OPT}
    \begin{aligned}
    OPT(\bm{C}, d, \gamma): \quad \max \quad & \sum_{i = 1}^{M} \sum_{j = 1}^{N} r_{i} x_{ij} \\
    \text {s.t.} \quad & \sum_{j=1}^{N} x_{ij} + x_{i0} = d_{i}, \quad i \in \mathcal{M},  \\ 
    & x_{i j} \leq \sum_{\bm{h} \in S(c_{j})} h_i y_{j \bm{h}}, \quad i \in \mathcal{M}, j \in \mathcal{N}, \\
    & \sum_{\bm{h} \in S(c_{j})} y_{j \bm{h}} \leq 1, \quad j \in \mathcal{N} \\
    & x_{ij} \geq \gamma_{ij}, \quad i \in \mathcal{M}, j \in \mathcal{N} \cup \{0\}.
\end{aligned}
\end{equation}

Note that comparing to the primal, we add a set of constraints $x_{ij} \geq \gamma_{ij}$ for all $i \in \mathcal{M}, j \in \mathcal{N} \cup \{0\}$. It is clear that \eqref{improve_primal} equals $OPT(\bm{C}, d, 0)$.

We use superscripts to denote realized values from a sample path. Specifically, for $t_1 \leq t_2$, let $d_{i}^{[t_1, t_2]}$ denote the realized demand for type $i$ during $[t_1, t_2]$, and let $\gamma_{ij}^{[t_1, t_2)}$ denote the number of type $i$ requests allocated to knapsack $j$ by the DPP during $[t_1, t_2)$. The total realized demand over the horizon is therefore $d^{[1, T]}$. The total expected demand over the horizon is expressed by $\lambda^{[1, T]}$.

$OPT(\bm{C}, d^{[1, T]}, \gamma^{[1,t+1)})$ can be interpreted as the total reward obtained under a virtual policy where we first follow the DPP during $[1, t+1)$ and then from time $t+1$ we follow the optimal solution assuming that we know the future demands. Let $x_{i^{t}j^{t}}^{*}(t)$ denote the optimal solution for $\text{OPT}(\bm{C}({t}), d^{[t, T]}, 0)$ at time $t$, where $i^{t}$ indicates the arriving type at time $t$, $j^{t}$ indicates the assigned knapsack at time $t$. 

The following lemma shows an important property of \eqref{OPT}.

\begin{lem}\label{additive}
    Given $\hat{d}$, for any $1 \leq t_1 \leq t_2 \leq T+1$, we have
    $$OPT(\bm{C}({1}), \hat{d} + d^{[1, t_2)} , \gamma^{[1, t_2)}) = \sum_{i} r_{i} \sum_{j} \gamma_{ij}^{[1, t_1)} + OPT(\bm{C}({t_1}), \hat{d}+d^{[t_1, t_2)}, \gamma^{[t_1, t_2)})$$
\end{lem}

For one sample path $\omega$, the HO under $\omega$ is $OPT(\bm{C}, d^{[1, T]}, 0)$. From Lemma \ref{additive}, by setting $\hat{d} = 0$ and $t_1 = t_2 = T+1$, we can see that the total revenue collected under $\omega$ is $OPT(\bm{C}, d^{[1, T]}, \gamma^{[1, T]})$. Therefore, the loss incured by DPP for $\omega$ can be written as

\begin{align*}
    & OPT^{\omega}(\bm{C}, d^{[1, T]}, 0) - OPT^{\omega}(\bm{C}, d^{[1, T]}, \gamma^{[1, T]}) \\
 = & \sum_{t=1}^{T} [OPT^{\omega}(\bm{C}, d^{[1,T]}, \gamma^{[1,t)}) - OPT^{\omega}(\bm{C}, d^{[1,T]}, \gamma^{[1,t+1)})]
\end{align*}

The revenue loss between HO and DPP can be decomposed into $T$ increments. Each increment represents the gap between two ``adjacent'' OPTs.

In the second step, we show that the gap can be uniformly bounded by a constant $\delta$ related to $\{r_{M}\}$. ({\color{blue} unrelated to problem scale, need to be proved.}) For any $\omega$, $OPT^{\omega}(\bm{C}, d^{[1, T]}, \gamma^{[1,t)}) - OPT^{\omega}(\bm{C}, d^{[1, T]}, \gamma^{[1,t+1)}) \leq \delta$.

The expected revenue loss can be upper bounded:

\begin{align*}
    & E_{\omega}[OPT^{\omega}(\bm{C}, d^{[1, T]}, 0) - OPT^{\omega}(\bm{C}, d^{[1, T]}, \gamma^{[1, T]})] \\
 \leq & \delta \sum_{t=1}^{T} P(OPT^{\omega}(\bm{C}, d^{[1, T]}, \gamma^{[1,t)}) - OPT^{\omega}(\bm{C}, d^{[1, T]}, \gamma^{[1,t+1)}) > 0) \\
 = & \delta \sum_{t=1}^{T} P(OPT^{\omega}(\bm{C}({t}), d^{[t, T]}, 0) - OPT^{\omega}(\bm{C}({t}), d^{[t, T]}, \gamma^{[t,t+1)}) > 0) \\
 \leq & \delta \sum_{t=1}^{T} P(x_{i^{t}j^{t}}^{*}(t) <1)
\end{align*}

The first inequality results from $E[A] \leq \delta E[\bm{1}_{A>0}] = \delta P(A>0)$. The second equation is as follows. If $x_{i^{t}j^{t}}^{*}(t) \geq 1$, then $x^{*}(t)$ is still feasible for $OPT(\bm{C}({t}), d^{[t, T]}, \gamma^{[t,t+1)})$. (Because the optimal policy)

In the third step, we consider $P(x_{i^{t}j^{t}}^{*}(t) <1)$. Let $x_{i^t j^{t}}^{\mathrm{DPP}}(t)$ denote the number of type $i^{t}$ assigned to knapsack $j^{t}$ for DPP. At time period $t$, after realization of $i^{t}$, based on the maximum choice of $j^{t}$ in DPP, we have

$$
x_{i^t j^t}^{\mathrm{DPP}}(t)=\max_{j} x_{i^t j}^{\mathrm{DPP}}(t) \geqslant \frac{\sum_{j} x_{i^t j}^{\mathrm{DPP}}(t)}{\sum_{j} 1} = \frac{\lambda_{i^t}^{[t, \theta T]}}{N+1}
$$

The inequality holds because the maximum value over $j$ is always greater than or equal to the average value.
 

% At time $t$, solve the primal problem \eqref{improve_primal} with demands $d_{i} = \sum_{\tau = t}^{T} * \lambda_{i}^{\tau}$. If the resulting solution has $x_{i} > 0$ for the current request's type $i$, then accept the request.

% Let $T_{i} = \sup\{t \leq T: \lambda_{i}^{t}> 0\}$. 

% Then 
% $$
% \inf_{\substack{t, i: \\ t \in\left[\theta T_{i}\right]}} \frac{\lambda_{i}^{\left(t, \theta T_{i}\right]}(\theta)}{\theta T_{i}-t}
% $$
% is lower bounded by some positive constant 
% $$
% \lambda_{\min } \triangleq \inf_{i} \frac{\lambda_{i}^{T_{i}}}{T_{i}}
% $$


% The first equation follows from Lemma \ref{additive}. (Let $t_1 = t_2 = t$, $\hat{d} = d^{[t, T]}$; let $t_1 = t, t_2 = t+1$, $\hat{d} = d^{[t+1, T]}$).

The bound on $\mathbb{P}\left(x_{i^{t}j^{t}}^{*}(t) <1\right)$ converges to the constant is presented in the Appendix.
\newpage