% !TEX root = sum1.tex
\section{Asymptotic Loss}

% \begin{lem}
%     Loss: $\lim_{\theta \to \infty} V_{\theta}^{\text{DP}} - z_{\theta}({\text{BPC}}) = 0$. 
% \end{lem}

\begin{prop}
    Loss: $V_{\theta}^{\text{HO}} - V_{\theta}^{\text{BL}} = O(\sqrt{\theta})$. 
\end{prop}

\begin{prop}
    Loss: $V_{\theta}^{\text{HO}} - V_{\theta}^{\text{P}} = O(1)$. 
\end{prop}

% Let $T_{i} = sup \{t \leq T: \lambda_{i}^{t} > 0\}$. 

% \newpage

\subsection{Loss for Static BLC Policy}
Booking limit control policy:

\begin{align}
    \quad \max \quad & \sum_{i=1}^{M}  \sum_{j= 1}^{N} r_{i} x_{ij} \label{theta_deter} \\
    \text {s.t.} \quad & \sum_{j= 1}^{N} x_{ij} \leq d_{i}, \quad i \in \mathcal{M}, \notag \\ 
    & \sum_{i=1}^{M} w_{i} x_{ij} \leq L_j, j \in \mathcal{N}, \notag 
\end{align}


Let $d_{i} =\sum_{t} \bm{1}_{i_{t} = i}$ be the realized number of type $i$ over the horizon $T$. 
Consider the deterministic linear programming \eqref{theta_deter}, where the right-hand side demands are set to $\sum_{t} \lambda_{i}^{t}$. Let $x_{ij}^{*}$ be an integral optimal solution to this LP, and define $d_{i}^{*} = \sum_{j} x_{ij}^{*}$ as the total number of type $i$ assigned in this optimal solution. Finally, let $val(I; \{d_{i}\})$ denote the optimal objective value of \eqref{theta_deter} for a given instance $I$ and demand vector $\{d_{i}\}$.

$V^{BL}(I) = E_{\{d_{i}\}}[\sum_{i} r_{i} \min\{d_{i}^{*}, d_{i}\}]$, $V^{OPT}(I) = E_{\{d_{i}\}} [val(I; \{d_{i}\})] \leq val(I; \{E[d_{i}]\})$.

$val(I; \{d_{i}\})$ is concave in $d_{i}$.

\begin{align*}
   & V^{OPT}(I) - V^{BL}(I) \\
\leq & val(I; \{E[d_{i}]\}) - V^{BL}(I) \\
= & val(I; \{E[d_{i}]\}) - val(I; \{\lfloor E[d_{i}]\rfloor\}) + val(I; \{\lfloor E[d_{i}]\rfloor\}) - E_{\{d_{i}\}}[\sum_{i} r_{i} \min\{d_{i}^{*}, d_{i}\}] \\
\leq & \sum_{i} r_{i} + N \sum_{i} r_i + E_{\{d_{i}\}}[\sum_{i} r_{i} (d_{i}^{*} - \min\{d_{i}^{*}, d_{i}\})] \\
= & (1+N) \sum_{i} r_{i} + E_{\{d_{i}\}}[\sum_{i} \frac{1}{2} r_{i} (d_{i}^{*} - d_{i} + |d_{i}^{*} - d_{i}|)] \\
\overset{\text{(a)}}{\leq} & (1+N) \sum_{i} r_{i} + \frac{1}{2} \sum_{i} r_{i} (d_{i}^{*} - E[d_{i}] + |d_{i}^{*} - E[d_{i}]| + \sqrt{\Var[d_{i}]}) \\
\leq & (1+N) \sum_{i} r_{i} + \frac{1}{2} \sum_{i} r_{i} \sqrt{\Var[d_{i}]} \\
\leq & (1+N) \sum_{i} r_{i} + \frac{1}{2} \sum_{i} r_{i} \sqrt{T p_{i} (1- p_{i})} = O(\sqrt{T})
\end{align*}

The revenue loss between the static deterministic heuristic and the optimal is bounded by $C \sqrt{T}$.
Thus, $\lim_{T \to \infty} (V^{OPT}(I) - V^{BL}(I))/T \to 0$.

$val(I; \{E[d_{i}]\}) - val(I; \{\lfloor E[d_{i}]\rfloor\}) \leq val(I; \{\lceil E[d_{i}]\rceil\}) - val(I; \{\lfloor E[d_{i}]\rfloor\}) \leq \sum_{i} r_{i}$


$LP -IP \leq \sum_{i} \sum_{j} r_{i} (x_{ij}^{*} - \lfloor x_{ij}^{*} \rfloor) < N \sum_{i} r_i$ $\Rightarrow$ $val(I; \{\lfloor E[d_{i}]\rfloor\}) \leq IP + N \sum_{i} r_i$.

$IP = \sum_{i} \sum_{j} r_{i} x_{ij}^{*} = \sum_{i} r_{i} d_{i}^{*}$

$(a)$ results from the following inequalities: $|d_{i}^{*} -d_{i}| = |(d_{i}^{*}-E[d_{i}]) + (E[d_{i}] -d_{i})| \leq |d_{i}^{*}-E[d_{i}]| + |d_{i} - E[d_{i}]|$. Take the expectation, we have $E[|d_{i}^{*} -d_{i}|]\leq |d_{i}^{*}-E[d_{i}]| + E[|d_{i} - E[d_{i}]|]$. $E[|d_{i} - E[d_{i}]|] \leq \sqrt{\Var[d_{i}]}$(Since $E[|X|] \leq \sqrt{E[X^{2}]}$). $d_{i}^{*} \leq E[d_{i}]$.

% 0-1 multiple

% \begin{align}
%     \quad \max \quad & \sum_{i=1}^{M}  \sum_{j= 1}^{N} p_i x_{ij} \\
%     \text {s.t.} \quad & \sum_{i= 1}^{M} w_{i} x_{ij} \leq L_{j}, \quad j \in \mathcal{N} \\ 
%     & \sum_{j=1}^{N} x_{ij} \leq 1, i \in \mathcal{M}  \\
%     & x_{ij} \in \{0,1\}, \quad i \in \mathcal{M}, j \in \mathcal{N}. 
% \end{align}

% Here, $M = \sum_{i=1}^{m} d_{i}$ represents the number of groups. $p_{k} = (n_{i} - \delta), w_{k} = n_{i}$ if group $k$ belongs to type $i$.


% single-leg RM: bid-price and booking limit expected revenue loss of $O(\sqrt{k})$ even with re-solving.


\subsection{Loss for Dynamic Primal Policy}

$E[\text{loss}] = V^{\text{off}} - V_{\pi}^{on} \geq V^{\text{opt}} - V_{\pi}^{on}$

Let $V^{\text{OPT}}(I)$ denote the expected value under offline optimal policy (relaxed) during $T$ periods for instance $I$ (capacity, probability distribution).


Let $\gamma_{ij}$, $\gamma_{i}^{0}$ denote the number of type $i$ accepted in capacity $j$ and rejected by this policy, respectively.


% Re-solving (each stage) bid-price (DLP) is equivalent to the optimal policy.
\begin{align*}
    OPT(\bm{C}, d, \gamma): \quad \max \quad & \sum_{i = 1}^{M} \sum_{j = 1}^{N} r_{i} x_{ij} \\
    \text {s.t.} \quad & \sum_{j=1}^{N} x_{ij} + x_{i0} = d_{i}, \quad i \in \mathcal{M},  \\ 
    & x_{ij} \geq \gamma_{ij}, \quad i \in \mathcal{M}, j \in \mathcal{N} \\
    & x_{i0} \geq \gamma_{i}^{0}, \quad i \in \mathcal{M}, \\
    & \sum_{i=1}^{M} w_{i} x_{ij} \leq c_{j}, \quad j \in \mathcal{N}.
\end{align*}


% At time $t$, solve the primal problem \eqref{improve_primal} with demands $d_{i} = \sum_{\tau = t}^{T} * \lambda_{i}^{\tau}$. If the resulting solution has $x_{i} > 0$ for the current request's type $i$, then accept the request.


$d^{[1, T]}$ is the demand realization during $[1, T]$. $\gamma_{ij}^{[1, t)}$ represents the number of requests $i$ assigned in knapsack $j$ by the policy during $[1, t)$.

$OPT(\bm{C}, d^{[1, T]}, \gamma^{[1,t+1)})$ can be interpreted as the total reward obtained under a virtual policy where we first follow the heuristic policy during $[1, t+1)$ and then from time $t+1$ we follow the optimal solution assuming that we know the future demands.

For one sample path of the requests, the revenue loss can be decomposed into $T$ increments.

\begin{align*}
    & OPT(\bm{C}, d^{[1, T]}, 0) - OPT(\bm{C}, d^{[1, T]}, \gamma^{[1, T]}) \\
 = & \sum_{t=1}^{T} [OPT(\bm{C}, d^{[1,T]}, \gamma^{[1,t)}) - OPT(\bm{C}, d^{[1,T]}, \gamma^{[1,t+1)})]
\end{align*}

% Let $c_{j}^{t} = c_{j} -\sum_{i=1}^{M} w_{i} \gamma_{ij}^{[1,t)}$. 

Let $T_{i} = \sup\{t \leq T: \lambda_{i}^{t}> 0\}$. 

Then 
$$
\inf_{\substack{t, i: \\ t \in\left[\theta T_{i}\right]}} \frac{\lambda_{i}^{\left(t, \theta T_{i}\right]}(\theta)}{\theta T_{i}-t}
$$

is lower bounded by some positive constant 
$$
\lambda_{\min } \triangleq \inf_{i} \frac{\lambda_{i}^{T_{i}}}{T_{i}}
$$


The expected revenue loss can be upper bounded:

\begin{align*}
    & E[OPT(\bm{C}, d^{[1, T]}, 0) - OPT(\bm{C}, d^{[1, T]}, \gamma^{[1, T]})] \\
 \leq & l \sum_{t=1}^{T} P(OPT(\bm{C}, d^{[1, T]}, \gamma^{[1,t)}) - OPT(\bm{C}, d^{[1, T]}, \gamma^{[1,t+1)}) > 0) \\
 = & l \sum_{t=1}^{T} P(OPT(\bm{C}^{t}, d^{[t, T]}, 0) - OPT(\bm{C}^{t}, d^{[t, T]}, \gamma^{[t,t+1)}) > 0) \\
 \leq & l \sum_{t=1}^{T} P(x_{i^{t}j^{t}}^{*,t} <1)
\end{align*}

$OPT(\bm{C}, d^{[1, T]}, \gamma^{[1,t)}) - OPT(\bm{C}, d^{[1, T]}, \gamma^{[1,t+1)}) \leq l$.
$l$ is related to $r_{M}$.

\begin{lem}\label{additive}
$OPT(\bm{C}^{1}, \hat{d} + d^{[1, t_2)} , \gamma^{[1, t_2)}) = \sum_{i} r_{i} \sum_{j} \gamma_{ij}^{[1, t_1)} + OPT(\bm{C}^{t}, \hat{d}+d^{[t_1, t_2)}, \gamma^{[t_1, t_2)})$
\end{lem}

For any optimal solution $x^{*}$ of $OPT(\bm{C}^{t}, \hat{d}+d^{[t_1, t_2)}, \gamma^{[t_1, t_2)})$, $x^{*} + \gamma^{[1, t_1)}$ is a feasible solution of $OPT(\bm{C}^{1}, \hat{d}+d^{[1, t_2)}, \gamma^{[1, t_2)})$. For any optimal solution $x^{*}$ of $OPT(\bm{C}^{1}, \hat{d}+d^{[1, t_2)}, \gamma^{[1, t_2)})$, $x^{*}- \gamma^{[1, t_1)}$ is a feasible solution of $OPT(\bm{C}^{t}, \hat{d}+d^{[t_1, t_2)}, \gamma^{[t_1, t_2)})$ because $x^{*}- \gamma^{[1, t_1)} \geq \gamma^{[1, t_{2})}- \gamma^{[1, t_1)} = \gamma^{[t_1, t_2)}$.


The first inequality results from $E[A] \leq l E[\bm{1}_{A>0}] = l P(A>0)$.

The first equation follows from Lemma \ref{additive}. (Let $t_1 = t_2 = t$, $\hat{d} = d^{[t, T]}$; let $t_1 = t, t_2 = t+1$, $\hat{d} = d^{[t+1, T]}$).

The second equation is as follows. If $x_{i^{t}j^{t}}^{*,t} \geq 1$, then $x^{*,t}$ is still feasible for $OPT(\bm{C}^{t}, d^{[t, T]}, \gamma^{[t,t+1)})$. (Because the optimal policy)

$x_{i^{t}j^{t}}^{*,t}$ is the optimal solution for $\text{OPT}(\bm{C}^{t}, d^{[t, T]}, 0)$ at time $t$.

Now we consider $P(x_{i^{t}j^{t}}^{*,t} <1)$. In time period $t$, after realization of $i^{t}$, based on the maximum choice of $j^{t}$ in dynamic primal policy, we have

$$
\gamma_{i^t j^t}^{\mathrm{P}, t}=\max_{j} \gamma_{i^t j}^{\mathrm{P}, t} \geqslant \frac{\sum_{j} \gamma_{i^t j}^{\mathrm{P}, t}}{\sum_{j} 1} = \frac{\lambda_{i^t}^{[t, \theta T]}}{N}
$$


$$
\begin{aligned}
& \mathbb{P}\left(x_{i^{t}j^{t}}^{*,t} <1\right) \\
\leqslant & \mathbb{P}\left(x_{i^{t}j^{t}}^{*,t} < x_{i^{t}j^{t}}^{\mathrm{P}, t}+1-\frac{\lambda_{i^t}^{[t, \theta T]}}{N}\right) \\
= & \mathbb{P}\left(x_{i^{t}j^{t}}^{\mathrm{P}, t} - x_{i^{t}j^{t}}^{*,t} > \frac{\mathbb{E}\left[d_{i^t}^{[t, \theta T]}\right]-N}{N}\right) \\
\stackrel{(\mathrm{a})}{\leqslant} & \mathbb{P}\left(\max_{i^{\prime}}\left|\mathbb{E}\left[d_{i^{\prime}}^{[t, \theta T]}\right]-d_{i^{\prime}}^{[t, \theta T]}\right|> \frac{\mathbb{E}\left[d_{i^t}^{[t, \theta T]}\right]-N}{\delta N} \right) \\
= & \mathbb{P}\left(\bigcup_{i^{\prime}}\left\{\left|\mathbb{E}\left[d_{i^{\prime}}^{[t, \theta T]}\right] - d_{i^{\prime}}^{[t, \theta T]}\right|> \frac{\mathbb{E}\left[d_{i^t}^{[t, \theta T]}\right]-N}{\delta N} \right\}\right) \\
\leqslant & \sum_{i^{\prime}} \mathbb{P}\left(\left|\mathbb{E}\left[d_{i^{\prime}}^{[t, \theta T]}\right]-d_{i^{\prime}}^{[t, \theta T]}\right|> \frac{\mathbb{E}\left[d_{i^t}^{[t, \theta T]}\right]-N}{\delta N}\right) \\
\stackrel{(\mathrm{b})}{=} & \sum_{i^{\prime}} \sum_{i} \mathbb{P}\left(\left.\left|\mathbb{E}\left[d_{i^{\prime}}^{[t, \theta T]}\right]-d_{i^{\prime}}^{[t, \theta T]}\right| > \frac{\mathbb{E}\left[d_{i^t}^{[t, \theta T]}\right]-N}{\delta N} \right\rvert\,\left(i^t = i\right) \right) \mathbb{P}\left(i^t = i \right)  \\
\leqslant & \sum_{i} \sum_{i^{\prime}} \mathbb{P}\left(\left.\left|\mathbb{E}\left[d_{i^{\prime}}^{[t, \theta T]}\right]-d_{i^{\prime}}^{[t, \theta T]}\right| > \frac{\mathbb{E}\left[d_{i^t}^{[t, \theta T]}\right]-N}{\delta N} \right\rvert\,\left(i^t = i\right) \right) \bm{1}\left\{t \leqslant \theta T_{i}\right\} \\
\stackrel{(\mathrm{c})}{\leqslant} & \sum_{i} \sum_{i^{\prime}} \mathbb{P}\left(\left.\left|\mathbb{E}\left[d_{i^{\prime}}^{[t, \theta T]}\right]-d_{i^{\prime}}^{[t, \theta T]}\right| > \frac{\mathbb{E}\left[d_{i^t}^{[t, \theta T]}\right]-N}{\delta N} \right\rvert\,\left(i^t = i\right) \right) \bm{1}\left\{t \leqslant \theta T_{i}\right\} \\
\stackrel{(\mathrm{d})}{=} & \sum_{i} \sum_{i^{\prime}} \mathbb{P}\left(\left|\mathbb{E}\left[d_{i^{\prime}}^{[t, \theta T]}\right]-d_{i^{\prime}}^{[t, \theta T]}\right| > \frac{\mathbb{E}\left[d_{i^t}^{[t, \theta T]}\right]-N}{\delta N} \right) \bm{1}\left\{t \leqslant \theta T_{i}\right\}.
\end{aligned}
$$




% The loss can be divided with capacity loss and decision loss.